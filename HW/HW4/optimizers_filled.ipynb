{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:14.412602Z",
     "iopub.status.busy": "2025-08-17T17:22:14.412324Z",
     "iopub.status.idle": "2025-08-17T17:22:14.422615Z",
     "shell.execute_reply": "2025-08-17T17:22:14.421892Z",
     "shell.execute_reply.started": "2025-08-17T17:22:14.412554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math, time, copy, os, sys\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Determinism\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:14.423522Z",
     "iopub.status.busy": "2025-08-17T17:22:14.423341Z",
     "iopub.status.idle": "2025-08-17T17:22:14.438859Z",
     "shell.execute_reply": "2025-08-17T17:22:14.438054Z",
     "shell.execute_reply.started": "2025-08-17T17:22:14.423507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        from array import array\n",
    "        import struct\n",
    "        \n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            assert magic == 2049, f'Expected 2049, got {magic}'\n",
    "            labels = np.frombuffer(file.read(), dtype=np.uint8)\n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            assert magic == 2051, f'Expected 2051, got {magic}'\n",
    "            images = np.frombuffer(file.read(), dtype=np.uint8).reshape(size, rows, cols)\n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test   = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:14.440599Z",
     "iopub.status.busy": "2025-08-17T17:22:14.440299Z",
     "iopub.status.idle": "2025-08-17T17:22:14.458229Z",
     "shell.execute_reply": "2025-08-17T17:22:14.457495Z",
     "shell.execute_reply.started": "2025-08-17T17:22:14.440562Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==== FIXED HYPERPARAMETERS (DO NOT CHANGE) ====\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Optimizer hyperparams\n",
    "HP = {\n",
    "    \"momentum\": {\"lr\": 0.05, \"momentum\": 0.9},\n",
    "    \"amsgrad\": {\"lr\": 0.03, \"beta1\": 0.9, \"beta2\": 0.999, \"eps\": 1e-8},\n",
    "    \"rmsprop\": {\"lr\": 1e-3, \"alpha\": 0.99, \"eps\": 1e-8},\n",
    "    \"adam\": {\"lr\": 1e-3, \"beta1\": 0.9, \"beta2\": 0.999, \"eps\": 1e-8},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:14.459072Z",
     "iopub.status.busy": "2025-08-17T17:22:14.458868Z",
     "iopub.status.idle": "2025-08-17T17:22:15.117203Z",
     "shell.execute_reply": "2025-08-17T17:22:15.116436Z",
     "shell.execute_reply.started": "2025-08-17T17:22:14.459025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loader = MnistDataloader(\n",
    "    \"/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte\",\n",
    "    \"/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte\",\n",
    "    \"/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\",\n",
    "    \"/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\"\n",
    ")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = loader.load_data()\n",
    "\n",
    "# Convert to torch tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test  = torch.tensor(x_test, dtype=torch.float32).unsqueeze(1) / 255.0\n",
    "y_test  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Full train dataset\n",
    "full_train = TensorDataset(x_train, y_train)\n",
    "\n",
    "# Make a train/val split (55k / 5k)\n",
    "train_set, val_set = random_split(full_train, [55000, 5000])\n",
    "\n",
    "# Test dataset\n",
    "test_set = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(len(train_set), len(val_set), len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:15.118281Z",
     "iopub.status.busy": "2025-08-17T17:22:15.117994Z",
     "iopub.status.idle": "2025-08-17T17:22:15.142161Z",
     "shell.execute_reply": "2025-08-17T17:22:15.141515Z",
     "shell.execute_reply.started": "2025-08-17T17:22:15.118255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==== MODEL (fixed small CNN) ====\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=0)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=0)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.drop1 = nn.Dropout(0.25)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        # For 28x28 -> after two 3x3 convs (no pad), size 24x24 -> pool -> 12x12 -> 64ch -> 64*12*12 = 9216\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.drop1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def accuracy(logits, targets):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "model = MNISTNet().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:15.144508Z",
     "iopub.status.busy": "2025-08-17T17:22:15.144065Z",
     "iopub.status.idle": "2025-08-17T17:22:15.166758Z",
     "shell.execute_reply": "2025-08-17T17:22:15.166006Z",
     "shell.execute_reply.started": "2025-08-17T17:22:15.144485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BaseOptimizer:\n",
    "    def __init__(self, params):\n",
    "        # Expect a list of parameter tensors with .data and .grad\n",
    "        self.params = [p for p in params if p.requires_grad]\n",
    "        self.state = {}  # dict[param] = per-param buffers\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.zero_()\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSGD(BaseOptimizer):\n",
    "    def __init__(self, params, lr=0.05, momentum=0.9):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr          # η\n",
    "        self.momentum = momentum  # γ\n",
    "        for p in self.params:\n",
    "            self.state[p] = {\"v\": torch.zeros_like(p)}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad\n",
    "            st = self.state[p]\n",
    "            v = st[\"v\"]\n",
    "            # v_t = γ v_{t-1} + η g_t\n",
    "            v.mul_(self.momentum).add_(g, alpha=self.lr)\n",
    "            # θ ← θ - v_t\n",
    "            p.add_(v, alpha=-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMSGrad(BaseOptimizer):\n",
    "    def __init__(self, params, lr=0.03, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(params)\n",
    "        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps\n",
    "        for p in self.params:\n",
    "            self.state[p] = {\n",
    "                \"t\": 0,\n",
    "                \"m\": torch.zeros_like(p),\n",
    "                \"v\": torch.zeros_like(p),\n",
    "                \"vmax\": torch.zeros_like(p)  # \\tilde v_t\n",
    "            }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        b1, b2, eps = self.beta1, self.beta2, self.eps\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad\n",
    "            st = self.state[p]\n",
    "            st[\"t\"] += 1\n",
    "            t = st[\"t\"]\n",
    "            m, v, vmax = st[\"m\"], st[\"v\"], st[\"vmax\"]\n",
    "\n",
    "            # m_t, v_t\n",
    "            m.mul_(b1).add_(g, alpha=1 - b1)\n",
    "            v.mul_(b2).addcmul_(g, g, value=1 - b2)\n",
    "\n",
    "            # mhat_t, vhat_t\n",
    "            mhat = m / (1 - b1**t)\n",
    "            vhat = v / (1 - b2**t)\n",
    "\n",
    "            # \\tilde v_t = max(\\tilde v_{t-1}, vhat_t)\n",
    "            torch.maximum(vmax, vhat, out=vmax)\n",
    "\n",
    "            # θ update\n",
    "            p.addcdiv_(mhat, vmax.sqrt().add_(eps), value=-self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(BaseOptimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.9, eps=1e-8):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.eps = eps\n",
    "        for p in self.params:\n",
    "            self.state[p] = {\"Eg2\": torch.zeros_like(p)}  # E[g^2]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        a, eps = self.alpha, self.eps\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad\n",
    "            Eg2 = self.state[p][\"Eg2\"]\n",
    "            # E[g^2]_t = α E[g^2]_{t-1} + (1-α) g_t^2\n",
    "            Eg2.mul_(a).addcmul_(g, g, value=1 - a)\n",
    "            # θ ← θ - η g / sqrt(E[g^2]_t + ε)\n",
    "            denom = (Eg2 + eps).sqrt()\n",
    "            p.addcdiv_(g, denom, value=-self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(BaseOptimizer):\n",
    "    def __init__(self, params, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        super().__init__(params)\n",
    "        self.lr, self.beta1, self.beta2, self.eps = lr, beta1, beta2, eps\n",
    "        for p in self.params:\n",
    "            self.state[p] = {\"t\": 0, \"m\": torch.zeros_like(p), \"v\": torch.zeros_like(p)}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self):\n",
    "        b1, b2, eps = self.beta1, self.beta2, self.eps\n",
    "        for p in self.params:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            g = p.grad\n",
    "            st = self.state[p]\n",
    "            st[\"t\"] += 1\n",
    "            t = st[\"t\"]\n",
    "            m, v = st[\"m\"], st[\"v\"]\n",
    "\n",
    "            # m_t, v_t\n",
    "            m.mul_(b1).add_(g, alpha=1 - b1)\n",
    "            v.mul_(b2).addcmul_(g, g, value=1 - b2)\n",
    "\n",
    "            # bias-corrected\n",
    "            mhat = m / (1 - b1**t)\n",
    "            vhat = v / (1 - b2**t)\n",
    "\n",
    "            # θ update\n",
    "            p.addcdiv_(mhat, vhat.sqrt().add_(eps), value=-self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:15.167835Z",
     "iopub.status.busy": "2025-08-17T17:22:15.167530Z",
     "iopub.status.idle": "2025-08-17T17:22:15.187949Z",
     "shell.execute_reply": "2025-08-17T17:22:15.187341Z",
     "shell.execute_reply.started": "2025-08-17T17:22:15.167808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(model, loader, optimizer=None):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(is_train)\n",
    "    total_loss, total_acc, total_count = 0.0, 0.0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        if is_train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        bs = y.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc += (logits.argmax(dim=1) == y).float().sum().item()\n",
    "        total_count += bs\n",
    "\n",
    "    return total_loss / total_count, total_acc / total_count\n",
    "\n",
    "def train_one_optimizer(opt_name, OptClass, hp):\n",
    "    model = MNISTNet().to(DEVICE)\n",
    "    opt = OptClass(model.parameters(), **hp)\n",
    "    history = {\"time\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, opt)\n",
    "        va_loss, va_acc = run_epoch(model, val_loader, None)\n",
    "\n",
    "        t = time.time() - t0\n",
    "        history[\"time\"].append(t)\n",
    "        history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "        print(f\"[{opt_name}] epoch {epoch}/{EPOCHS}  \"\n",
    "              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f}  \"\n",
    "              f\"val_loss={va_loss:.4f} acc={va_acc:.4f}  time={t:.1f}s\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:22:15.189067Z",
     "iopub.status.busy": "2025-08-17T17:22:15.188757Z",
     "iopub.status.idle": "2025-08-17T17:25:17.516584Z",
     "shell.execute_reply": "2025-08-17T17:25:17.515935Z",
     "shell.execute_reply.started": "2025-08-17T17:22:15.189043Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"Momentum\": (MomentumSGD, HP[\"momentum\"]),\n",
    "    \"AMSGrad\": (AMSGrad, HP[\"amsgrad\"]),\n",
    "    \"RMSProp\": (RMSProp, HP[\"rmsprop\"]),\n",
    "    \"Adam\": (Adam, HP[\"adam\"]),\n",
    "}\n",
    "\n",
    "all_hist = {}\n",
    "for name, (OptClass, hp) in optimizers.items():\n",
    "    _, hist = train_one_optimizer(name, OptClass, hp)\n",
    "    all_hist[name] = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:25:17.517520Z",
     "iopub.status.busy": "2025-08-17T17:25:17.517301Z",
     "iopub.status.idle": "2025-08-17T17:25:18.848314Z",
     "shell.execute_reply": "2025-08-17T17:25:18.847634Z",
     "shell.execute_reply.started": "2025-08-17T17:25:17.517503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Plot: one page with 2x2 subplots; each subplot shows Train/Val Loss (left y) and Accuracy (right y) vs Epoch ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "order = [\"Momentum\", \"AMSGrad\", \"RMSProp\", \"Adam\"]\n",
    "\n",
    "for ax, name in zip(axes.ravel(), order):\n",
    "    hist = all_hist[name]\n",
    "    epochs = list(range(1, len(hist[\"train_loss\"]) + 1))\n",
    "    # Left y: Loss\n",
    "    ax.plot(epochs, hist[\"train_loss\"], label=\"Train Loss\")\n",
    "    ax.plot(epochs, hist[\"val_loss\"], label=\"Val Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(name)\n",
    "\n",
    "    # Right y: Accuracy\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(epochs, hist[\"train_acc\"], linestyle=\"--\", label=\"Train Acc\")\n",
    "    ax2.plot(epochs, hist[\"val_acc\"], linestyle=\"--\", label=\"Val Acc\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "\n",
    "# Single legend outside\n",
    "handles1, labels1 = axes[0,0].get_legend_handles_labels()\n",
    "handles2, labels2 = axes[0,0].twinx().get_legend_handles_labels()\n",
    "fig.legend(handles1 + handles2, labels1 + labels2, loc=\"upper center\", ncol=4)\n",
    "fig.suptitle(\"MNIST: Loss & Accuracy vs Epoch for Four Optimizers\", y=0.98)\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-17T17:25:18.849434Z",
     "iopub.status.busy": "2025-08-17T17:25:18.849110Z",
     "iopub.status.idle": "2025-08-17T17:25:18.872954Z",
     "shell.execute_reply": "2025-08-17T17:25:18.872381Z",
     "shell.execute_reply.started": "2025-08-17T17:25:18.849406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === Page-2 Summary Helper ===\n",
    "import pandas as pd\n",
    "rows = []\n",
    "for name in [\"Momentum\", \"AMSGrad\", \"RMSProp\", \"Adam\"]:\n",
    "    h = all_hist[name]\n",
    "    rows.append({\n",
    "        \"optimizer\": name,\n",
    "        \"final_train_acc\": h[\"train_acc\"][-1],\n",
    "        \"final_val_acc\": h[\"val_acc\"][-1],\n",
    "        \"final_train_loss\": h[\"train_loss\"][-1],\n",
    "        \"final_val_loss\": h[\"val_loss\"][-1],\n",
    "        \"total_time_s\": h[\"time\"][-1],\n",
    "    })\n",
    "summary_df = pd.DataFrame(rows)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plotting (single figure with 4 subplots) ===\n",
    "def plot_all(histories, save_path=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 9))\n",
    "\n",
    "    # Subplot 1: Train Accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(h[\"train_acc\"], label=name)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Train Acc\"); plt.title(\"Training Accuracy\"); plt.legend()\n",
    "\n",
    "    # Subplot 2: Val Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(h[\"val_acc\"], label=name)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Acc\"); plt.title(\"Validation Accuracy\"); plt.legend()\n",
    "\n",
    "    # Subplot 3: Train Loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(h[\"train_loss\"], label=name)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Train Loss\"); plt.title(\"Training Loss\"); plt.legend()\n",
    "\n",
    "    # Subplot 4: Val Loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(h[\"val_loss\"], label=name)\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Val Loss\"); plt.title(\"Validation Loss\"); plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "    # Also save to Kaggle working path if available\n",
    "    try:\n",
    "        plt.savefig(\"/kaggle/working/hw4_all_plots.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        plt.savefig(\"/mnt/data/hw4_all_plots.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    plt.show()\n",
    "\n",
    "# If histories dict 'all_hist' exists, plot now\n",
    "try:\n",
    "    _ = plot_all(all_hist, save_path=\"hw4_all_plots.png\")\n",
    "except NameError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 102285,
     "sourceId": 242592,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
